## 강의 내용 정리

mysql, postgre (production) -> Warehouse로 복사

Airflow 내의 Parameter를 설정해주는 기능이 있다. (Connection, Variable)

#### DAG

- Staetdate + interval로 실행됨.
ex) daily dag : 2020-07-26 01:00 은 2020-07-27 01:00 에 실행됨
    hourly dag : 2020-07-26 01:00 은 2020-07-26 02:00 에 실행


빅쿼리랑 Snowflake는 쓴 만큼 돈을 냄. (Redshift는 정액지출) -> join문, catchup설정 잘못 걸면 비용 커짐.

startdate 2020-08-10 02:00 인데 2020-08-13 on시키면? -> 과거 시간에 대해서 catch up함.
catchup False로 두면 14일 이후부터 시행됨.

max_active_runs -> dag가 여러개 도는 상황에서 한번에 돌릴지, 차례로 돌릴지 정할떄
concurrency -> 하나의 dag 안에서 몇개의 task를 돌게할지

#### PythonOperator

provide_context = True -> python function 내의 params기능을 쓰려면 True로 지정(python dictionary에서 인자를 가져올거임)
-> NameGenderCSVtoRedshift_v2.py


Xcom 객체를 사용하여 extract transform load를 각각 만든다, redshift table name도 parameter로 넘긴다.
-> NameGenderCSVtoRedshift_v3.py, NameGenderCSVtoRedshift_v4.py

장단점 : parameter가 바뀐다고 해서 코드 수정이 필요없음, 개인정보 노출 위험이 적음 BUT 실수할시 디버깅이 복잡해짐.

#### 터미널 내 DAG 실행 방법

airflow test Dag이름, task이름, 날짜
airflow test second_assignment_v3 extract 2020-08-09 (바로 실행시키고 싶으면 하루 과거로 잡아야됨)

#### ETL 소개

priduction postgres,mysql에 있는 정보들을 airflow를 통해서 copy하는 작업이 필요함.

product -> airflow server(local disk) -> s3 -> redshift bulk복사
